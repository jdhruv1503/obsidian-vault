\
In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), the **coefficient of determination**, denoted $R^2$ or $r^2$ and pronounced "R squared", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).

The variability of the data set can be measured with two [sums of squares](https://en.wikipedia.org/wiki/Mean_squared_error "Mean squared error") formulas:

- The sum of squares of residuals, also called the [residual sum of squares](https://en.wikipedia.org/wiki/Residual_sum_of_squares "Residual sum of squares"): S S res = ∑ i ( y i − f i ) 2 = ∑ i e i 2 ![{\displaystyle SS_{\text{res}}=\sum _{i}(y_{i}-f_{i})^{2}=\sum _{i}e_{i}^{2}\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2669c9340581d55b274d3b8ea67a7deb2225510b) 
- The [total sum of squares](https://en.wikipedia.org/wiki/Total_sum_of_squares "Total sum of squares") (proportional to the [variance](https://en.wikipedia.org/wiki/Variance "Variance") of the data): S S tot = ∑ i ( y i − y ¯ ) 2 ![{\displaystyle SS_{\text{tot}}=\sum _{i}(y_{i}-{\bar {y}})^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3a1f55d7e84c24299917fb3fec4d0439b81e728d) 

The most general definition of the coefficient of determination is R 2 = 1 − S S r e s S S t o t ![{\displaystyle R^{2}=1-{SS_{\rm {res}} \over SS_{\rm {tot}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/42b3cd78531d8b2f2590c7fda76acff7caeb643a) 

In the best case, the modeled values exactly match the observed values, which results in S S res = 0 ![{\displaystyle SS_{\text{res}}=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8d346fa0b307220ee37e11e0545b6a400e3b7006) and _R_2 = 1. A baseline model, which always predicts _y_, will have _R_2 = 0.