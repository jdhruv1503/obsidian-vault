\
In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), the **coefficient of determination**, denoted $R^2$ or $r^2$ and pronounced "R squared", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).

The variability of the data set can be measured with two [sums of squares](https://en.wikipedia.org/wiki/Mean_squared_error "Mean squared error") formulas:

- The sum of squares of residuals, also called the [residual sum of squares](https://en.wikipedia.org/wiki/Residual_sum_of_squares "Residual sum of squares"): $\displaystyle SS_{\text{res}}=\sum _{i}(y_{i}-f_{i})^{2}=\sum _{i}e_{i}^{2}$ 

- The [total sum of squares](https://en.wikipedia.org/wiki/Total_sum_of_squares "Total sum of squares") (proportional to the [variance](https://en.wikipedia.org/wiki/Variance "Variance") of the data): $SS_{\text{tot}}=\sum _{i}(y_{i}-{\bar {y}})^{2}$

The most general definition of the coefficient of determination is  $R^{2}=1-{SS_{\rm {res}} \over SS_{\rm {tot}}}$

In the best case, the modeled values exactly match the observed values, which results in $R_2 = 1$. A baseline model, which always predicts _y_, will have $R_2 = 0.$