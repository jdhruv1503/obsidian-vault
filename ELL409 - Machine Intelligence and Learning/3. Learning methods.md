
![[Clustering or unsupervised learning]]

## Classification problems

### Overview

We have to find a [[Hypothesis]] h in [[Hypothesis space]] H such that h(x) = c(x) for all x in D.

**Inductive learning hypothesis**: Any hypothesis approximating c over training samples will also approximate c over all samples.

### Find-S algorithm

#### General to specific ordering of hypotheses.

Definition: Let hj and hk be boolean-valued functions defined over X. Then hj is
moregeneral-than-or-equal-to hk (written hj 2, h k ) if and only if

![[Pasted image 20240812040516.png]]

This can be visualised using [[Hasse diagram]]

![[Hasse diagram]]

 One way is to begin
with the most specific possible hypothesis in H, then generalize this hypothesis
each time it fails to cover an observed positive training example. (We say that
a hypothesis "covers" a positive example if it correctly classifies the example as
positive.)

This is the **Find-S algo**:

![[Pasted image 20240812040737.png]]

Because we are moving from **the most specific** hypothesis and assume that $c$ exists in $H$, we never need to look at negative examples!! $c$ will always be less or equal specificity as compared to our current hypothesis, and so if training samples are well formed wrt $c$, we are guaranteed to never false-positive a negative example.

### Limitations about Find-S

• Has the learner converged to the current target concept?
• No way to determine if it has found the only hypothesis that is consistent with
the target concept
• Or there are many other consistent hypotheses as well
• Why prefer the most specific hypothesis?
• In case of multiple hypotheses consistent with the target concept, why to
consider the most specific one?
• Are the training example consistent?
• What if a few training instances are corrupted?
• What if there are several maximally specific consistent hypotheses?
• Find-S should be backtracked to generalize the hypothesis
